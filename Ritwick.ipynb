{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loading and Preprocessing Functions\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "\n",
    "def normalize_signals(X):\n",
    "    mean = np.mean(X, axis=1, keepdims=True)\n",
    "    std = np.std(X, axis=1, keepdims=True)\n",
    "    X_norm = (X - mean) / (std + 1e-8)\n",
    "    return X_norm\n",
    "\n",
    "def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "    return list(set(tmp))\n",
    "\n",
    "def split_data(X, Y):\n",
    "    test_fold = 10\n",
    "    X_train = X[Y.strat_fold != test_fold]\n",
    "    y_train = Y[Y.strat_fold != test_fold]['label'].values\n",
    "    X_test = X[Y.strat_fold == test_fold]\n",
    "    y_test = Y[Y.strat_fold == test_fold]['label'].values\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_dataloaders(X_train, y_train, X_test, y_test, batch_size=32):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # Shape: (batch_size, seq_len, channels)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Load Data\n",
    "path = '../dataset/'\n",
    "sampling_rate = 100\n",
    "\n",
    "Y = pd.read_csv(path + 'ptbxl_database.csv', index_col='ecg_id')\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "X = load_raw_data(Y, sampling_rate, path)\n",
    "\n",
    "agg_df = pd.read_csv(path + 'scp_statements.csv', index_col=0)\n",
    "agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)\n",
    "\n",
    "# Filter and Encode Labels\n",
    "has_superclass = Y['diagnostic_superclass'].apply(lambda x: len(x) > 0)\n",
    "X = X[has_superclass.values]\n",
    "Y = Y[has_superclass]\n",
    "\n",
    "# Assign a single superclass label (choose the first one)\n",
    "Y['superclass_label'] = Y['diagnostic_superclass'].apply(lambda x: x[0])\n",
    "\n",
    "# Map superclasses to integer labels\n",
    "superclasses = sorted(Y['superclass_label'].unique())\n",
    "superclass_to_int = {k: v for v, k in enumerate(superclasses)}\n",
    "int_to_superclass = {v: k for k, v in superclass_to_int.items()}\n",
    "Y['label'] = Y['superclass_label'].map(superclass_to_int)\n",
    "num_classes = len(superclasses)\n",
    "\n",
    "print(\"Diagnostic superclasses and their corresponding labels:\")\n",
    "for k, v in superclass_to_int.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Split Data\n",
    "X_train, y_train, X_test, y_test = split_data(X, Y)\n",
    "\n",
    "# Normalize Data\n",
    "X_train = normalize_signals(X_train)\n",
    "X_test = normalize_signals(X_test)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader, test_loader = create_dataloaders(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Model Definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, bidirectional=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Define the output layer\n",
    "        direction = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size * direction, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_len, hidden_size * num_directions)\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size * num_directions)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(out)  # (batch_size, num_classes)\n",
    "        return out\n",
    "\n",
    "# Training Parameters\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[2]  # Number of features (channels)\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = len(superclasses)\n",
    "bidirectional = True  # Set to True for a bidirectional LSTM\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes, bidirectional).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Function\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Move data to the appropriate device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "                y_true.extend(y_batch.numpy())\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Accuracy: {acc*100:.2f}%, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Train the Model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
