{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "InceptionModule1D.__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 154\u001b[0m\n\u001b[1;32m    152\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m input_channels \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mInceptionModule1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    155\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    156\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: InceptionModule1D.__init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import ast\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Data loading and preprocessing functions\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data\n",
    "\n",
    "def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            tmp.append(agg_df.loc[key].diagnostic_class)\n",
    "    return list(set(tmp))\n",
    "\n",
    "def split(X, Y):\n",
    "    test_fold = 10\n",
    "    # Train\n",
    "    X_train = X[np.where(Y.strat_fold != test_fold)]\n",
    "    y_train = Y[Y.strat_fold != test_fold].diagnostic_superclass\n",
    "    # Test\n",
    "    X_test = X[np.where(Y.strat_fold == test_fold)]\n",
    "    y_test = Y[Y.strat_fold == test_fold].diagnostic_superclass\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def encode_filter(X_train, y_train, X_test, y_test, permute=True):\n",
    "    unique_classes = list(set([item for sublist in y_train for item in sublist]))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "    # Filter data\n",
    "    y_train_filtered = [labels for labels in y_train if labels]\n",
    "    X_train_filtered = X_train[np.array([bool(labels) for labels in y_train])]\n",
    "    y_test_filtered = [labels for labels in y_test if labels]\n",
    "    X_test_filtered = X_test[np.array([bool(labels) for labels in y_test])]\n",
    "\n",
    "    y_train_encoded = [class_to_idx[labels[0]] for labels in y_train_filtered]\n",
    "    y_test_encoded = [class_to_idx[labels[0]] for labels in y_test_filtered]\n",
    "\n",
    "    if permute:\n",
    "        X_train_tensor = torch.tensor(X_train_filtered, dtype=torch.float32).permute(0, 2, 1)\n",
    "        X_test_tensor = torch.tensor(X_test_filtered, dtype=torch.float32).permute(0, 2, 1)\n",
    "    else:\n",
    "        X_train_tensor = torch.tensor(X_train_filtered, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test_filtered, dtype=torch.float32)\n",
    "\n",
    "    y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, len(unique_classes), class_to_idx\n",
    "\n",
    "# Main code execution\n",
    "if __name__ == '__main__':\n",
    "    path = ''  # Set your dataset path here\n",
    "    sampling_rate = 100\n",
    "\n",
    "    # Load and convert annotation data\n",
    "    Y = pd.read_csv(path + 'ptbxl_database.csv', index_col='ecg_id')\n",
    "    Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    # Load raw signal data\n",
    "    X = load_raw_data(Y, sampling_rate, path)\n",
    "\n",
    "    # Load scp_statements.csv for diagnostic aggregation\n",
    "    agg_df = pd.read_csv(path + 'scp_statements.csv', index_col=0)\n",
    "    agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "\n",
    "    # Apply diagnostic superclass\n",
    "    Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, y_train, X_test, y_test = split(X, Y)\n",
    "\n",
    "    # Encode labels and prepare data loaders\n",
    "    train_loader, test_loader, num_classes, class_to_idx = encode_filter(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Define the corrected InceptionTime model\n",
    "    class InceptionModule1D(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_sizes=[9, 19, 39], bottleneck_channels=32, use_residual=True):\n",
    "            super(InceptionModule1D, self).__init__()\n",
    "            self.use_residual = use_residual\n",
    "\n",
    "            # Adjust bottleneck_channels if in_channels is less than bottleneck_channels\n",
    "            self.bottleneck_channels = min(in_channels, bottleneck_channels)\n",
    "\n",
    "            # Bottleneck layer\n",
    "            self.bottleneck = nn.Conv1d(in_channels, self.bottleneck_channels, kernel_size=1, bias=False)\n",
    "\n",
    "            # Convolutional layers with adjusted input channels\n",
    "            self.conv1 = nn.Conv1d(self.bottleneck_channels, out_channels, kernel_size=kernel_sizes[0],\n",
    "                                   padding=kernel_sizes[0] // 2, bias=False)\n",
    "            self.conv2 = nn.Conv1d(self.bottleneck_channels, out_channels, kernel_size=kernel_sizes[1],\n",
    "                                   padding=kernel_sizes[1] // 2, bias=False)\n",
    "            self.conv3 = nn.Conv1d(self.bottleneck_channels, out_channels, kernel_size=kernel_sizes[2],\n",
    "                                   padding=kernel_sizes[2] // 2, bias=False)\n",
    "            self.maxpool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
    "            self.conv4 = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "            # Batch normalization and activation\n",
    "            self.bn = nn.BatchNorm1d(out_channels * 4)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "            # Residual connection\n",
    "            if self.use_residual and in_channels != out_channels * 4:\n",
    "                self.residual = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels * 4, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm1d(out_channels * 4)\n",
    "                )\n",
    "            else:\n",
    "                self.residual = nn.Identity()\n",
    "\n",
    "        def forward(self, x):\n",
    "            input_residual = x\n",
    "\n",
    "            x = self.bottleneck(x)\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x)\n",
    "            x3 = self.conv3(x)\n",
    "            x4 = self.conv4(self.maxpool(input_residual))\n",
    "\n",
    "            x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "            x = self.bn(x)\n",
    "\n",
    "            if self.use_residual:\n",
    "                x += self.residual(input_residual)\n",
    "            x = self.relu(x)\n",
    "            return x\n",
    "\n",
    "    class InceptionTime(nn.Module):\n",
    "        def __init__(self, in_channels, num_classes, num_modules=6):\n",
    "            super(InceptionTime, self).__init__()\n",
    "            modules = []\n",
    "            for i in range(num_modules):\n",
    "                if i == 0:\n",
    "                    modules.append(InceptionModule1D(in_channels, 32))\n",
    "                else:\n",
    "                    modules.append(InceptionModule1D(128, 32))\n",
    "            self.inception_modules = nn.Sequential(*modules)\n",
    "            self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "            self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.inception_modules(x)\n",
    "            x = self.global_avg_pool(x).squeeze(-1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    # Instantiate the model, define loss and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_channels = X_train.shape[2]\n",
    "    model = InceptionTime(in_channels=input_channels, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training function with metrics computation\n",
    "    def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            y_pred, y_true = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in test_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    y_pred.extend(predicted.cpu().numpy())\n",
    "                    y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "            micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "            macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "            macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "            print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "            print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "            print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "            print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "            print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10)\n",
    "\n",
    "    # Compute detailed classification report after training\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Get class names in the correct order\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    target_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute ROC curves and AUC\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from itertools import cycle\n",
    "    import numpy as np\n",
    "\n",
    "    # Binarize the labels for ROC computation\n",
    "    n_classes = num_classes  # Number of classes\n",
    "    all_labels_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "    # Collect probabilities and true labels\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probabilities.cpu().numpy())\n",
    "            all_labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    # Binarize the labels\n",
    "    all_labels_bin = label_binarize(all_labels, classes=range(n_classes))\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels_bin.ravel(), all_probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red'])\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'ROC curve of class {target_names[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    # Plot micro-average ROC curve\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})',\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([-0.05, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
